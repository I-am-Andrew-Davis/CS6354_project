\documentclass[conference]{IEEEtran}
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage[hyphens]{url}

\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

% Ensure letter paper
\pdfpagewidth=8.5in
\pdfpageheight=11in

%%%%%%%%%%%---SETME-----%%%%%%%%%%%%%
\newcommand{\iscasubmissionnumber}{NaN}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\pagenumbering{arabic}

%%%%%%%%%%%---SETME-----%%%%%%%%%%%%%
\title{A Survey of Graphical Processing Units}
\author{\normalsize{Inji Kim and Kavish Ranawella and Andrew Davis}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}
\maketitle
\thispagestyle{plain}
\pagestyle{plain}



%%%%%% -- PAPER CONTENT STARTS-- %%%%%%%%

\begin{abstract}
  This paper is a survey on Graphical Processing Units (GPUs).
  The sections were divided the following way: The introduction and conclusion were collaborative.
  Inji Kim wrote the section on Architecture, Kavish Ranawella wrote the section on Simulators, and Andrew Davis wrote the section on Memory Models.
\end{abstract}

\section{Introduction}
GPUs are powerful accelerators that are becoming widely used in all aspects of computing, such as machine learning, computational fluid dynamics, image processing, and other general purpose computing on graphics processing units (GPGPU) tasks.
The underlying architecture, memory models, and simulator tools are both similar and distinct from a traditional central processing unit (CPU).
In this survey of GPUs, we will discuss the underlying architecture, memory models, and the current state of the art GPU simulators.
We will also discuss some of the state of the art improvements to the underlying architecture, memory models, and GPU simulators.

The Architecture section explores GPU structure and optimization techniques, focusing on key components such as the Shader Core and hierarchical memory. It examines performance bottlenecks like memory access issues, resource imbalance, and branch divergence, and discusses solutions including scheduling optimizations and Thread Block Compaction (TBC).
It also highlights tools for enhancing GPU performance, such as GASS (an open-source GPU compiler), a GPU ISA decoding platform, Xscope (a floating-point exception detection framework), and neural network acceleration methods like VectorSparse and Tensor Core.

The memory models section is organized as follows: an overview of the hierarchy of GPU on chip memory, i.e. caches, shared memory, etc, is discussed, followed by a brief overview of off chip memory, i.e. DRAM.
Some of the state of the art implementations of GPU memory models to be discussed are: memory-coalescing strategies, private vs shared cache design, synchronization strategies, cache coherence, specific findings of underlying memory hardware, virtual memory, and virtual memory addressing schemes.

The simulators section will discuss the importance of investing considerable research effort into developing open-source simulators, examine the current state-of-the-art simulators and highlight the benefits they have provided so far.
The focus will be on how these simulators need to be updated to model the modern GPUs, with most examples drawn from NVIDIA's ecosystem.
The overall flow will illustrate how these simulators have been progressively extended, increasing their generalizability and applicability over time.
\section{Architecture}

NVIDIA GPUs are architected to maximize parallel processing, with the fundamental unit being the Shader Core. A Shader Core consists of multiple SIMD pipelines. The CUDA model utilizes the GPU as a co-processor specialized in parallel computation, where threads grouped into CTAs (Cooperative Thread Arrays) collaboratively perform tasks using shared memory.

GPUs manage data through a hierarchical memory structure composed of L1 cache, shared memory, and global memory. The fastest memory is the L1 cache connected to each Shader Core, which works alongside local and shared memory for rapid data processing. Shared memory within the Streaming Multiprocessor (SM) allows data exchange among CTAs within the SM, reducing data processing bottlenecks. Global memory, though slower, is accessible by all cores and serves as a large-capacity storage for handling extensive datasets. Data transfers occur via an interconnect network, ensuring efficient connectivity between Shader Cores and memory controllers ~\cite{Bakhoda2009}.

The GPU scheduling mechanism supports multiple levels of parallelism, starting with warps (groups of 32 threads) as the smallest unit, followed by CTAs (thread blocks) as the intermediate level, and CUDA streams at the top level. Workloads are executed within Streaming Multiprocessors (SMs), where tasks are queued and processed ~\cite{Sanudo2020}.

GPU performance can be hindered by memory access bottlenecks, resource imbalance, and branch divergence. Memory bottlenecks occur when parallel memory requests within a warp are inefficiently handled. Excessive CTA execution can lead to memory resource contention. Branch divergence arises when threads within a warp follow different branches, reducing parallel processing efficiency. To mitigate these issues, scheduling algorithms need to be optimized to evenly distribute tasks across SMs and to better coalesce memory requests.

The scheduling structure is divided into three main levels, each responsible for decisions at different granularities:
  1.Application Scheduler: At the top level, it manages task allocation across applications running on the GPU. Applications generate one or more channels and insert them into a runlist, where TDMA (Time Division Multiple Access) methods are used for scheduling based on priority and time slice length.
  2.Stream Scheduler: Within each application, it manages tasks submitted by the application across multiple CUDA streams. Tasks within a stream are executed in FIFO order, and CUDA allows the setting of stream priorities.
  3.Block Scheduler: Maps CUDA task blocks to individual Streaming Multiprocessors (SMs) to maximize parallelism. Work blocks are distributed primarily using a round-robin method. Within each SM, blocks are decomposed into warps, which are scheduled using the Loose Round Robin (LRR) approach. When a warp is stalled due to memory dependencies, another ready warp is scheduled. Key resources such as registers, shared memory, and warps are prioritized during block scheduling.
Parallel execution of tasks across streams can improve performance, but synchronization or resource contention complicates timing predictability ~\cite{Bakhoda2009}, ~\cite{Sanudo2020}.

To alleviate branch divergence, the Thread Block Compaction (TBC) mechanism has been proposed. TBC addresses divergence in the SIMT (Single-Instruction, Multiple-Thread) execution model by dynamically reorganizing threads within a block to group those following similar paths into new "compacted warps." These newly formed warps execute in parallel and restore the original static warp configuration at the next reconvergence point.

TBC replaces the traditional per-warp reconvergence stack with a block-wide reconvergence stack, enabling efficient synchronization and compaction by tracking control flow across all threads in a block. This mechanism minimizes idle threads, improves SIMD efficiency, and reduces memory divergence, thereby enhancing memory access efficiency. Additionally, TBC resolves scheduling inefficiencies and excessive memory conflicts encountered in prior methods like Dynamic Warp Formation (DWF). It eliminates complex hardware requirements and prevents execution bottlenecks such as starvation eddies.

Experiments show that TBC achieves a 22\% performance improvement over the per-warp stack mechanism (PDOM) and a 17\% improvement over DWF, particularly excelling in applications with significant control flow divergence, such as graph traversal or molecular dynamics simulations. TBC is a robust and efficient solution that enhances GPU performance while minimizing hardware overhead ~\cite{Fung2011HPC}.

GASS, the pioneering open-source LLVM-based compiler, represents a significant milestone in GPU programming by enabling the conversion of LLVM-IR directly into SASS code for NVIDIA GPUs. This breakthrough addresses a longstanding limitation in the field, as previous tools like NVIDIA's proprietary ptxas were closed-source, limiting transparency and customizability. GASS offers compatibility across NVIDIA architectures, including Volta, Turing, and Ampere, creating an unprecedented platform for open experimentation and optimization. By modeling GPU hardware comprehensively, GASS facilitates fine-grained control over resource usage, supporting detailed representations of registers, vector predicates, and instruction sets. Its tailored instruction scheduling and if-conversion techniques further ensure efficient exploitation of computational parallelism and reduce data hazards by improving instruction latency and throughput ~\cite{Yan2022}.

The compiler's innovative contributions lie in its ability to directly address performance-critical aspects of GPU programming. For instance, if-conversion minimizes branch divergence by translating short branches into predicated instructions, boosting matrix multiplication throughput by up to 23\%. Meanwhile, its advanced instruction scheduler optimizes compute-bound kernels, yielding performance gains of 5-23\% over existing solutions. These optimizations empower developers to explore new research directions, such as register allocation strategies, branch divergence analysis, and compile-time instrumentation. GASSâ€™s open-source nature democratizes GPU performance tuning and creates opportunities for deeper exploration of architectural nuances, making it an invaluable tool for both academic and industrial use in high-performance computing and machine learning applications ~\cite{Yan2022}.

Hayes et al. outlined a framework for decoding NVIDIA GPU's closed ISA (Instruction Set Architecture), designed to support various architectures such as Fermi, Kepler, Maxwell, and Pascal. The framework enables bidirectional conversion between binary instructions and assembly code, systematically decoding the encoding structure of GPU instructions. This capability allows researchers to analyze microarchitectural features like instruction scheduling, memory management, and control flow in detail. Using automated ISA analysis and bit-flipping techniques, it detects instruction variations and generates assemblers compatible with newer GPU generations. This approach overcomes the limitations of traditional ISA analysis, providing broad GPU architecture compatibility ~\cite{Hayes2019}.

The framework bridges the gap between GPU software and hardware, playing a vital role in performance optimization, security analysis, and architectural research. Decoded ISAs facilitate tasks like performance tuning, binary modification, and instruction scheduling optimization, including converting local memory instructions to shared memory instructions or reconfiguring code blocks. This helps overcome research constraints imposed by NVIDIA GPU's closed architecture, supporting next-generation GPU hardware and software development.

Laguna et al. proposed Xscope, a novel framework for identifying floating-point exceptions caused by input values in NVIDIA GPUs. While GPUs excel at parallel computation, tools to detect or predict exceptions in floating-point operations (e.g., NaN, INF, underflow) are limited. NVIDIA GPUs lack hardware flags for exception detection. Xscope uses Bayesian Optimization to explore input value spaces of GPU functions, achieving a 72\% success rate in identifying exception-causing inputs for CUDA Math Library and HPC programs. This provides insights into floating-point issues in GPU architectures and offers solutions ~\cite{Laguna2022}.

Zhu et al. (2019) proposed a solution to improve the computational efficiency of neural networks by enabling faster execution of sparse neural networks on GPUs. Sparse neural networks reduce the computational workload by eliminating unnecessary values (zeros). However, GPUs are primarily optimized for dense neural networks, which consist mostly of non-zero values, leading to suboptimal performance for sparse networks. To address this, the researchers developed a novel algorithm called VectorSparse. This algorithm overcomes the limitation of dense networks, which lack removable values, and improves the computational efficiency of sparse networks. The method involves dividing the data into smaller vectors and removing values at the same ratio within each vector. Instead of simply discarding small numbers, the algorithm retains the most important values in each vector while setting the rest to zero. This approach maintains balanced sparsity, preventing workload imbalances that typically arise in GPUs when processing sparse networks in parallel ~\cite{Zhu2019}.

In addition, the researchers enhanced Tensor Core, a specialized hardware component in modern GPUs, to maximize the efficiency of sparse neural networks. Tensor Core is primarily optimized for dense data, but the team modified its instruction set and architecture to skip zeros and compute only the necessary values for sparse data. As a result, the algorithm alone achieved a 63\% performance improvement over dense neural networks, and further hardware optimizations provided an additional 58\% speedup. This study demonstrates a promising approach to processing complex neural networks more efficiently, highlighting significant potential for applications in machine learning and related fields~\cite{Zhu2019}.

\section{Memory Models}
GPU memory models have a similar design to CPU memory models in the sense that there is a hierarchy of memory, however, there are key differences from that of a CPU.
GPUs typically have a register file, L1 and L2 cache, and DRAM (Global) memories, these can be thought of as similar to a CPUs.
Additionally, GPUs typically have a shared memory and a constant/texture memory, as well as an interconnect network that connects to the CPU.
The register file, L1 and L2 cache, and shared memory are located on the GPU chip, while the constant/texture and DRAM memory are located off chip.
GPUs also have multiple levels of translational lookaside buffers (TLBs) to decrease the time for converting a virtual address to a physical one.

GPUs make use of many streaming multiprocessors (SMs), which can be thought of as similar to a core of a CPU.
Each SM has a private L1 cache and a shared memory.
All of the SMs share a L2 cache and the DRAM memory.
Within a SM are many threads, each thread has its own private register file.
One of the primary issues with GPUs is handling the large amount of memory requests to DRAM.
This problem is usually solved via memory coalescing~\cite{Jog2013OWL, Bakhoda2009, Singh2013, Fung2011HPC, Kadam2018} in which memory requests from threads requesting contiguous memory from DRAM are ``coalesced''.

There are two types of memory coalescing, intra-warp and inter-warp.
These two types of coalescing both increase memory-bandwidth, though they are implemented in different ways.
Inter-warp memory coalescing involves combining memory request from threads in different warps, this has been shown to significantly decrease memory traffic for data-dependent applications~\cite{Bakhoda2009}.
Intra-warp memory coalescing involves combining memory request from threads in the same warp.

However, memory coalescing makes a GPU more prone to correlation timing attacks~\cite{Kadam2018}. Kadam et al.~\cite{Kadam2018} proposed randomizing the coalescing logic for improved security at the cost of performance degradation.
They found that randomizing the granularity of the intra-warp coalescing and randomizing the allocation of thread elements per subwarp improved security while having a performance loss of 29 to 76\%.
This work only focused on intra-warp coalescing, but the authors suggest it could be implemented in all levels of the memory hierarchy.

As previously mentioned, GPU L1 caches are typically private.
This design has the drawback of duplication data, i.e. two separate L1 caches might contain the same information.
Ibrahim et al. investigated a shared L1 cache design, in which only one, unique, copy of data was stored in only one L1 cache~\cite{Ibrahim2020}.
Ibrahim et al. found that a shared L1 cache design achieved higher cache hit rates at the cost of latency to some applications.
This was achieved by mapping specific ranges of the address space to specific L1 caches, ensuring no duplicated data could exists.
The additional latency was due to increased communication of cores that needed data in other L1 caches.
Overall, performance increased between 22 and 52\%.

Lock-based synchronizations are very common in highly parallel applications.
However, GPUs have a tendency to deadlock when using lock-based synchronization.
Fung et al. proposed a hardware transactional memory model for GPUs, called Kilo transactional memory~\cite{Fung2011ISM}.
Transactional memory works by executing ``transactions'' of code that execute atomically rather than using locks.
It gives the programmer the illusion that no synchronizations are needed.
The trade off for this type of approach is that additional hardware must be implemented.
The hardware determines if there were any conflicts when executing the transaction, if there were, the code block is executed again until no conflicts are detected.
Kilo transactional memory works at the word-level, this is in contrast to other transactional models that work at the cache-line level.
The authors found that this approach increased performance of most applications.

Cache coherence is the concept that all copies of data in individual caches are consistent.
Cache coherent models are well established, and many high-level languages depend on these models~\cite{Singh2013}.
Unfortunately, GPUs lack cache coherence due to their architectural design~\cite{Singh2013}, and applications that rely on coherent caches must disable the private L1 cache.
Obviously, disabling the L1 cache to achieve coherence comes at a cost of performance.

Singh et al. investigated a method called ``Temporal Coherence'' that provides coherent L1 caches and can improve execution time of GPGPU applications by 85\%. This is a timestamp based coherence framework, which helps to eliminate coherence traffic caused by 100s of threads accessing memory and helps eliminate protocol races~\cite{Singh2013}.
However, coherent L1 caches would be gotten implicity with a shared L1 cache design previously mentioned by Ibrahim et al.~\cite{Ibrahim2020}.

One of the many challenges of understanding GPUs is understanding precisely how the memory hierarchy works.
This is a difficult task, as many GPU manufacturers do not make this information publicly available.
Mei and Chu investigated GPU memory hierarchies with a novel microbenchmarking approach~\cite{Mei2015}.
They specifically investigated global, shared, and texture memory, and found important characteristics of the Fermi, Kepler and Maxwell architectures.
Mei and Chu found that the Fermi and Kepler devices have a 12KB set-associative L1 texture cache with 4 cache sets that store 384 cache lines.
The Maxwell architecture was similar, except it could store 768 cache lines.
All 3 architectures do not use an LRU replacement policy, and have a cache line size of 32 bytes.
For shared memory, they found that all devices had 32 memory banks, with Fermi and Maxwell having a bank width of 4 bytes, and Kepler having a bank width of 8 bytes.
Interestingly, they also found that the efficiency of shared memory throughput was, 57.4\% on Fermi, 32.5\% on Kepler and 83.9\% on Maxwell.
Suggesting that a bank size of 4 bytes is optimal.
They also suggest that bank conflicts are the primary reason for poor memory latency when using shared memory on the GPU.

Virtual memory gives the illusion of a larger memory space than is physically available.
This design does not burden programmers with having to track physical memory.
GPUs now use multi-chip modules (MCM) to increase performance, however, this design can have implications on the virtual memory~\cite{Pratheek2023}.
Pratheek et al. explores the ideal design of a virtual memory system for a GPU that utilizes MCM.
They find that the partitioned nature of a MCM design leads to different design, and not one design works for all applications.
Mainly, the chief design consideration is to have a shared L2 TLB or a private L2 TLB.
Pratheek et al. proposed a MCM-aware virtual memory system which, "achieves a balance between using aggregate L2 TLB capacity, local L2 TLB accesses, and local page table entry accesses on a page walk.''~\cite{Pratheek2023}
This balance is achieved through intelligent scheduling of warps and data placement in MCM GPUs.

One of the problems with designing efficient address mapping schemes for GPUs is accounting for bit entropy~\cite{Liu2018}.
Liu et al.~\cite{Liu2018} found that the address bit entropy distribution did not follow a typical CPU bit entropy distribution.
Bit entropy is defined as high if the bit is likely to change and low if the bit is not likely to change.
Liu et al. notes that the entropy distribution is highly application dependent, this is in contrast to a CPU, in which typically the least significant bit is likely to have the highest entropy.
Additionally, these distributions tend to change while the application is running.
This problem is due to the GPUs highly parallelized architecture, in which thousands of threads might be accessing memory addresses concurrently.

The ideal memory addressing scheme would have row bits change the least, for row buffer locality, while the channel and bank bits should change the most, for uniform memory distribution~\cite{Liu2018}.
Liu et al. propsed the ``Page Address Entropy'' (PAE) mapping scheme, which fulfills these criteria, is easily translated into hardware with an array of XOR gates, and results in performance gains of around 1.25.

Due to the memory performance variability of applications on GPUs, it is often the case that algorithms need specific tuning to perform well on GPUs.
Liu et al.~\cite{Liu2020} shows this clearly with an example of how to increase performance of Non-deterministic Finite Automata (NFA). NFAs are naturally parallel, though they do not perform well on GPUs. Liu et al. found the main cause of this is excessive data movement and poor-compute utilization. This section will only talk about the data movement findings.

Liu et al. solved the data movement issue by privatizing reads using the GPUs on-chip memory.
They also made further optimizations that improve the NFA algorithm on the GPU by reducing redundant memory, reducing global memory accesses, and cleverly turning read operations into computations.
This example shows that, often algorithms that would theoretically perform well on the GPU (NFAs), need to be reworked in order to take full advantage of the GPUs highly parallel memory structure.

In summary, GPU memory systems share similarities with CPU memory systems, but also have stark differences.
Ideas like a hierarchy of memory are similar to a CPU, but the specific policies of the hierarchy of memory, such as a private or public L1 cache, are typically different than a CPU.
Much work has gone into finding the ideal memory system for GPUs, such as address mapping policies, write-through or write-back policies, coherent caches, and memory coalescing are all active areas of this research.
\section{Simulators}

Simulators play a huge role in enabling impactful research in the field of GPUs.
Identifying research gaps, tuning hardware configuration, evaluating the effect of new algorithms/architectures and design space exploration are some of the use cases for simulators.
While there have been many attempts to build simulators for GPU by multiple research groups, some have been more successful than the others.
One such simulator that had a long-lasting presence is GPGPU-Sim.

NVIDIA had been able to dominate over other GPU vendors after the release of NVIDIA's CUDA programming model combined with NVIDIA's Parallel Thread eXecution (PTX) virtual ISA, which made it easier for programmers to run parallelized programs.
Hence, work by Bakhoda et al.~\cite{Bakhoda2009} on extending the GPGPU-Sim to support Cuda made a huge impact on modern GPU research by being the base used by many simulators.
GPGPU-Sim is a performance simulator capable of giving cycle-level details which is much needed for aggressive research efforts.
At the very early ages of CUDA evolution, Bakhoda et al.~\cite{Bakhoda2009} evaluates 12 low-performing CUDA programs to identify why they don't perform as expected.
In their search, they identified that the performance is impacted more by the interconnect bisection bandwidth available than the latency used.
Moreover, it indicated that reducing memory system contentions by running a lower number concurrent threads than available on-chip resources has a considerable impact on performance.~\cite{Bakhoda2009}

GPGPU-Sim was relevant for a very long time, and many researchers extended it to evaluate many modern releases of GPUs.
A good example for this is the work by Raihan et al.~\cite{Raihan2018} where the functional and timing models were changed to model Tensor Cores.
With Deep Neural Networks (DNNs) becoming more popular across multiple fields, computational power expected from GPUs were increased.
Tensor Cores were introduced into GPUs, to compute multi-dimensional matrix calculations in the form of GEMM operations.
However, the underlying design of the Tensor Cores was not released by NVIDIA.
Hence, Raihan et al.~\cite{Raihan2018} extended GPGPU-Sim to run applications built with NVIDIAâ€™s CUTLASS library, which is an opensource CUDA C++ library used to direct GEMM operations to the Tensor Cores.

However, these open-source simulators are built on numerous baseline assumptions, since the simulators used in the industry or the exact details of these industrial designs are not available to the public~\cite{Khairy2020}.
In these instances, as there involves a high level of parallelism, researchers try to assume them as blackboxes or as simplified structures that is far off from the real systems.
One such detail that NVIDIA has kept discrete, is how their GPU hardware scheduler subdivides CUDA streams into streaming multiprocessors (SMs).
Sanudo et al.~\cite{Sanudo2020} explains how assumptions made by most simulators about the scheduling algorithm is wrong.
For example, a round-robin (RR) block-to-SM distribution policy can be considered for simplicity, but it gets a lot more complicated when there are multiple streams.
It is shown how this could lead to severe mispredictions of the block execution time.
When the number of streams increases, the probability of misprediction can go as high as 96\%.
This severly affects the researcher's ability to get insights from the system timing analysis since the reliability of these models are questionable.

GPUs makes use machine ISAs and virtual ISAs to abstract its hardware complexity.
NVIDIA's virtual ISA is Parallel Thread eXecution (PTX) and machine ISA is Source and ASSembly (SASS).
PTX ISA is very well documented and is close to the programming language and is machine independent.
However, information about the compilers or the more hardware-specific SASS ISA is not openly available.
This hinders research on optimizing compilers and binary encoding for specific applications.
NVIDIA provides a closed-source compiler called nvcc that converts PTX code to generate a binary and a closed-source disassembler called cuobjdump to convert the binary to SASS assembly code.
Still, there's no assemblers provided to convert SASS assembly code to binaries.
Identifying this gap, Hayes et al.~\cite{Hayes2019} developed a framework that automates the creation of an assembler, given the binary.
This was used to analyze the binary encoding of multiple GPU architectures.
Unlike the previous attempts, this is "a comprehensive and cross-architecture framework useful for general purpose applications"~\cite{Hayes2019}.

According to the current trends, the industry is innovating the hardware at a very fast pace, but since most of their designs are proprietory, the research world is having a hard time keeping up with them~\cite{Khairy2020}.
This is especially the case, when the baseline assumptions made during research is well off.
Hence, it is crucial to study these industrial design before trying to address a research gap, in order to keep the research still relevant to the time.
For example, Khairy et al.~\cite{Khairy2020} found that the L1 cache throughput bottleneck no longer exists in modern GPUs, making research techniques on selectively bypassing the L1 cache less relevant.

A main challenge with keeping up with the industry, is the constant changing of their undocumented machine ISA.
These changes can be drastic, which puts a huge burden on the researchers to add these changes to the open-source simulators to keep them up-to-date.
Accel-Sim~\cite{Khairy2020} tries to address this by introducing a flexible frontend where both virtual and machine ISA is converted to an ISA-independent intermediate representation that can be input to a performance model.
This functions in two modes; trace-driven and execution-driven modes where machine and virtual ISA codes are used respectively.
Just supporting trace-based simulations is not enough since they cannot properly evaluate new designs that rely on the data read from the registers / memory and global synchronization techniques~\cite{Khairy2020}.

Going beyond this, Accel-Sim extends GPGPU-Sim's performance model increasing its level of detail, configurability and accuracy.
This is rigorously validated by cross checking a set of counters outputted by the performance model against the hardware data emitted by NVIDIA's profilers.
This was able to reduce the "cycle error by 79 percentage points for 80 workloads consisting of 1,945 kernel instances"~\cite{Khairy2020}, which were the previous benchmarks used.
It further gave the capability to "simulate 60 additional workloads consisting of 11,440 kernel instances from the machine learning benchmark suite Deepbench"~\cite{Khairy2020}.
This comprehensive validation infrastructure was further extended to include a set of targeted GPU microbenchmarks, an automated parameter tuner and an automatic correlation visualizer accelerating the process of modeling new designs using Accel-Sim.

Among many insights gained through Accel-Sim, it also proved that some previously disregarded research areas may have hidden potential.
As the level of detail of the memory system was improved to match the newer hardware, it revealed that the out-of-order memory accesses can now offer significant performance improvements.

With the widespread adoption of GPUs in the accelerator space, energy efficiency became equally important as performance.
However, power modeling was not at the same level as performance modeling.
There weren't any detailed cycle-level power models, and static power calculation was over simplified in existing power models.
Older GPUs used fewer energy-efficency optimizations, and power models designed based on them can wrongly divert the attention to optimizing components that don't actually need energy optimizations~\cite{Kandiah2021}.
Modern GPUs have Dynamic Voltage and Frequency Scaling (DVFS).
They can have powered-down hardware components as well powered-up but inactive components.
Hence, using a constant for the static power will give huge inaccuracies.

Accel-Wattch~\cite{Kandiah2021} was developed to give a modern approach to power modeling.
It can be driven by a pure software performance model like Accel-Sim or hardware performance counters commonly found in modern GPUs.
By being able run using actual silicon gives it the capability of analyzing discrete hardware components without the need for developing performance models for them.
It manages to capture the cycle-level static and dynamic power, while "correctly modeling the effects of DVFS, thread divergence, intra-warp functional unit overlap, variable SM occupancy, and power gating"~\cite{Kandiah2021}.
Even though, it was modeled using the NVIDIA Volta GPU architecture, changing the GPU configuration to resemble the Pascal and Turing architectures gave accurate power models without any fine-tuning, proving its suitability for conducting design space explorations.

A more unconventional form of simulation is seen in the memory address entropy analysis by Liu et al.~\cite{Liu2018}.
To reach optimal performance in DRAMs, the low entropy bits should be mapped to row bits to exploit row buffer locality and high entropy bits should map to channel and bank bits to exploit parallelism.
When analyzed, entropy distribution for CPUs and GPUs were significantly different.
CPUs had higher entropy in lower bits, but GPUs showed an entropy valley effect in the lower bits.
Hence, using CPU-oriented address mappping schemes for GPU workloads will fail to give expected results.
Liu et al.~\cite{Liu2018} proposes Page Address Entropy (PAE) mapping scheme as the most power efficient address mapping scheme for GPUs.

In summary, modern GPUs incorporate numerous innovative optimization techniques that are often not publicly disclosed.
To advance research in this area, it is essential to model these designs accurately before proposing improvements.
By building upon and extending open-source simulation frameworks, the research community can collaboratively analyze the proprietory designs in greater depth, uncovering many of these hidden optimization techniques.

\section{Conclusion}
We have written an overview of the key components of the GPU, those being, the underlying architecture, the memory models, and the simulators used to test new GPU architecture ideas.
We have also relayed some of the state of the art findings from various academic papers on these concepts.
The following three paragraphs are a brief summary of the concepts detailed in Sections II to IV. The content is organized in the following way: architecture, memory models, and simulators.

The architecture of GPUs plays a crucial role in fields such as scientific research, artificial intelligence, and simulations. 
To overcome performance limitations and expand their range of applications, technologies like scheduling optimizations and Thread Block Compaction (TBC) have been introduced to address challenges such as memory bottlenecks, resource imbalance, and branch divergence.
The development of GPUs has significantly improved parallel computing efficiency. 
Open-source platforms like GASS enhance collaboration between hardware and software, while GPU ISA decoding platforms bridge gaps in closed architectures. 
Tools like Xscope and VectorSparse further optimize GPU efficiency and open new possibilities for applications.
NVIDIA GPUs have continuously evolved through ongoing hardware and software improvements. 
These advancements, supported by various tools and technologies, have further expanded the scope of GPU applications.

Inter-warp memory coalescing was found to be highly efficient by Bakhoda.
Ibrahim found performance gains when shifting from a private L1 cache to a shared L1 cache.
Fung describes a transactional memory policy that is superior to traditional lock-based synchronizations.
Singh explored a method that enabled cache coherence on the GPU, and found their method improved performance of some applications.
Mei and Chu deliver precise experimental findings of the Fermi, Kepler, and Maxwell GPU architectures that were not disclosed by NVIDIA; these findings are primarily on cache behavior, such as eviction policies and cache line size.
Pratheek found a virtual memory system design that improves memory performance for GPUs that use MCM technologies.
Lastly, Liu explored memory addressing modes that perform well on GPUs, these modes are application dependent and take into account an addresses bit entropy region.

Significant innovations in GPU technology comes from the industry, but many of the advancements are not disclosed to the public.
In order for research efforts to remain impactful, modern GPUs need to be accurately modeled through open-source simulators.
GPGPU-Sim laid the foundation for most modern GPU simulators, and these simulators use baseline assumptions to fill the knowledge gaps left by the industry.
However, incorrect baseline assumptions can misdirect research efforts and lead to incorrect conclusions.
With the increasing complexity of modern GPUs, it has become critical to simulate both detailed cycle-level performance and power models.
Developing generalized frameworks is essential to keep pace with the rapid evolution GPU hardware.
Tools like Accel-Sim and Accel-Wattch made significant advancements in this area, serving as game changers in accurately modeling modern GPUs.
These simulators have proven that oversimplified baseline assumptions can lead to misleading insights, emphasizing the need for more accurate and detailed modeling approaches.

\clearpage
%%%%%%%%% -- BIB STYLE AND FILE -- %%%%%%%%
\bibliographystyle{IEEEtranS}
\bibliography{refs}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\end{document}

