\section{Memory Models}
GPU memory models have a similar design to CPU memory models in the sense that there is a hierarchy of memory, however, there are key differences from that of a CPU.
GPUs typically have a register file, L1 and L2 cache, and DRAM (Global) memories, these can be thought of as similar to a CPUs.
Additionally, GPUs typically have a shared memory and a constant/texture memory, as well as an interconnect network that connects to the CPU.
The register file, L1 and L2 cache, and shared memory are located on the GPU chip, while the constant/texture and DRAM memory are located off chip.
GPUs also have multiple levels of transnational lookaside buffers (TLBs) to decrease the time for converting a virtual address to a physical one.

GPUs make use of many streaming multiprocessors (SMs), which can be thought of as similar to a core of a CPU.
Each SM has a private L1 cache and a shared memory.
All of the SMs share a L2 cache and the DRAM memory.
Within a SM are many threads, each thread has its own private register file.
One of the primary issues with GPUs is handling the large amount of memory requests to DRAM.
This problem is usually solved via memory coalescing~\cite{Jog2013OWL}~\cite{Bakhoda2009}~\cite{Singh2013}~\cite{Fung2011HPC}~\cite{Kadam2018} in which memory requests from threads requesting contiguous memory from DRAM are ``coalesced''.

There are two types of memory coalescing, intra-warp and inter-warp.
These two types of coalescing both increase memory-bandwidth, though they are implemented in different ways.
Inter-warp memory coalescing involves combining memory request from threads in different warps, this has been shown to significantly decrease memory traffic for data-dependent applications~\cite{Bakhoda2009}.
Intra-warp memory coalescing involves combining memory request from threads in the same warp.

However, memory coalescing makes a GPU more prone to correlation timing attacks~\cite{Kadam2018}. Kadam et al.~\cite{Kadam2018} proposed randomizing the coalescing logic for improved security at the cost of performance degradation.
They found that randomizing the granularity of the intra-warp coalescing and randomizing the allocation of thread elements per subwarp improved security while having a performance loss of 29 to 76\%.
This work only focused on intra-warp coalescing, but the authors suggest it could be implemented in all levels of the memory hierarchy.

As previously mentioned, GPU L1 caches are typically private.
This design has the drawback of duplication data, e.i. two separate L1 caches might contain the same information.
Ibrahim et al. investigated a shared L1 cache design, in which only one, unique, copy of data was stored in only one L1 cache~\cite{Ibrahim2020}.
Ibrahim found that this method achieved higher cache hit rates at the cost of latency to some applications.
This was achieved by mapping specific ranges of the address space to specific L1 caches.
The additional latency was due to increased communication of cores that needed data in other L1 caches.
Overall, performance increased between 22 and 52\%.

Lock-based synchronizations are very common in highly parallel applications.
However, GPUs have a tendency to deadlock when using lock-based synchronization.
Fung et al. proposed a hardware transactional memory model for GPUs, called Kilo transactional memory~\cite{Fung2011ISM}.
Transactional memory works by executing ``transactions'' of code that execute atomically rather than using locks.
It gives the programmer the illusion that no synchronizations are needed.
The trade off for this type of approach is that hardware must be implemented.
The hardware determines if there were any conflicts when executing the transaction, if there were, the code block is executed again until no conflicts are detected.
Kilo transactional memory works at the word-level, this is in contrast to other transactional models that work at the cache-line level.
The authors found that this approach increased performance of most applications.

Cache coherence is the concept that all copies of data in individual caches are consistent.
GPUs lack cache coherence due to their architectural design~\cite{Singh2013}, though cache coherence can be achieved by disabling private caches.
Obviously, disabling the L1 cache to achieve coherence comes at a cost of performance. Singh et al. investigated a method called ``Temporal Coherence'' that provides coherent L1 caches and can improve execution time of GPGPU applications by 85\%.

One of the many challenges of understanding GPUs is understanding precisely how the memory hierarchy works.
This is a difficult task, as many GPU manufacturers do not make this information publicly available.
Mei and Chu investigated GPU memory hierarchies with a novel microbenchmarking approach~\cite{Mei2015}.
They specifically investigated global, shared, and texture memory, and found important characteristics of the Fermi, Kepler and Maxwell architectures.
Mei and Chu found that the Fermi and Kepler devices have a 12KB set-associative L1 texture cache with 4 cache sets that store 384 cache lines.
The Maxwell architecture was similar, except it could store 768 cache lines.
All 3 architectures do not use an LRU replacement policy, and have a cache line size of 32 bytes.
For shared memory, they found that all devices had 32 memory banks, with Fermi and Maxwell having a bank width of 4 bytes, and Kepler having a bank width of 8 bytes.
Interestingly, they also found that the efficiency of shared memory throughput was, 57.4\% on Fermi, 32.5\% on Kepler and 83.9\% on Maxwell.
Suggesting that a bank size of 4 bytes is optimal.
They also suggest that bank conflicts are the primary reason for poor memory latency when using shared memory on the GPU.

Virtual memory gives the illusion of a larger memory space than is physically available.
This design does not burden programmers with having to track physical memory.
GPUs now use multi-chip modules (MCM) to increase performance, however, this design can have implications on the virtual memory~\cite{Pratheek2023}.
Pratheek et al. explores the ideal design of a virtual memory system for a GPU that utilizes MCM.
They find that the partitioned nature of a MCM design leads to different design, and not one design works for all applications.
Mainly, the chief design consideration is to have a shared L2 TLB or a private L2 TLB.
Pratheek et al. proposed a MCM-aware virtual memory system which, "achieves a balance between using aggregate L2 TLB capacity, local L2 TLB accesses, and local page table entry accesses on a page walk.''~\cite{Pratheek2023}
This balance is achieved through intelligent scheduling of warps and data placement in MCM GPUs.
