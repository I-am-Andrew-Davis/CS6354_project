\section{Memory Models}
GPU memory models have a similar design to CPU memory models in the sense that there is a hierarchy of memory, however, there are key differences from that of a CPU.
GPUs typically have a register file, L1 and L2 cache, and DRAM (Global) memories, these can be thought of as similar to a CPUs.
Additionally, GPUs typically have a shared memory and a constant/texture memory, as well as an interconnect network that connects to the CPU.
The register file, L1 and L2 cache, and shared memory are located on the GPU chip, while the constant/texture and DRAM memory are located off chip.
GPUs also have multiple levels of translational lookaside buffers (TLBs) to decrease the time for converting a virtual address to a physical one.

GPUs make use of many streaming multiprocessors (SMs), which can be thought of as similar to a core of a CPU.
Each SM has a private L1 cache and a shared memory.
All of the SMs share a L2 cache and the DRAM memory.
Within a SM are many threads, each thread has its own private register file.
One of the primary issues with GPUs is handling the large amount of memory requests to DRAM.
This problem is usually solved via memory coalescing~\cite{Jog2013OWL}~\cite{Bakhoda2009}~\cite{Singh2013}~\cite{Fung2011HPC}~\cite{Kadam2018} in which memory requests from threads requesting contiguous memory from DRAM are ``coalesced''.

There are two types of memory coalescing, intra-warp and inter-warp.
These two types of coalescing both increase memory-bandwidth, though they are implemented in different ways.
Inter-warp memory coalescing involves combining memory request from threads in different warps, this has been shown to significantly decrease memory traffic for data-dependent applications~\cite{Bakhoda2009}.
Intra-warp memory coalescing involves combining memory request from threads in the same warp.

However, memory coalescing makes a GPU more prone to correlation timing attacks~\cite{Kadam2018}. Kadam et al.~\cite{Kadam2018} proposed randomizing the coalescing logic for improved security at the cost of performance degradation.
They found that randomizing the granularity of the intra-warp coalescing and randomizing the allocation of thread elements per subwarp improved security while having a performance loss of 29 to 76\%.
This work only focused on intra-warp coalescing, but the authors suggest it could be implemented in all levels of the memory hierarchy.

As previously mentioned, GPU L1 caches are typically private.
This design has the drawback of duplication data, i.e. two separate L1 caches might contain the same information.
Ibrahim et al. investigated a shared L1 cache design, in which only one, unique, copy of data was stored in only one L1 cache~\cite{Ibrahim2020}.
Ibrahim et al. found that a shared L1 cache design achieved higher cache hit rates at the cost of latency to some applications.
This was achieved by mapping specific ranges of the address space to specific L1 caches, ensuring no duplicated data could exists.
The additional latency was due to increased communication of cores that needed data in other L1 caches.
Overall, performance increased between 22 and 52\%.

Lock-based synchronizations are very common in highly parallel applications.
However, GPUs have a tendency to deadlock when using lock-based synchronization.
Fung et al. proposed a hardware transactional memory model for GPUs, called Kilo transactional memory~\cite{Fung2011ISM}.
Transactional memory works by executing ``transactions'' of code that execute atomically rather than using locks.
It gives the programmer the illusion that no synchronizations are needed.
The trade off for this type of approach is that additional hardware must be implemented.
The hardware determines if there were any conflicts when executing the transaction, if there were, the code block is executed again until no conflicts are detected.
Kilo transactional memory works at the word-level, this is in contrast to other transactional models that work at the cache-line level.
The authors found that this approach increased performance of most applications.

Cache coherence is the concept that all copies of data in individual caches are consistent.
Cache coherent models are well established, and many high-level languages depend on these models~\cite{Singh2013}.
Unfortunately, GPUs lack cache coherence due to their architectural design~\cite{Singh2013}, and applications that rely on coherent caches must disable the private L1 cache.
Obviously, disabling the L1 cache to achieve coherence comes at a cost of performance.

Singh et al. investigated a method called ``Temporal Coherence'' that provides coherent L1 caches and can improve execution time of GPGPU applications by 85\%. This is a timestamp based coherence framework, which helps to eliminate coherence traffic caused by 100s of threads accessing memory and helps eliminate protocol races~\cite{Singh2013}.
However, coherent L1 caches would be gotten implicity with a shared L1 cache design previously mentioned by Ibrahim et al.~\cite{Ibrahim2020}.

One of the many challenges of understanding GPUs is understanding precisely how the memory hierarchy works.
This is a difficult task, as many GPU manufacturers do not make this information publicly available.
Mei and Chu investigated GPU memory hierarchies with a novel microbenchmarking approach~\cite{Mei2015}.
They specifically investigated global, shared, and texture memory, and found important characteristics of the Fermi, Kepler and Maxwell architectures.
Mei and Chu found that the Fermi and Kepler devices have a 12KB set-associative L1 texture cache with 4 cache sets that store 384 cache lines.
The Maxwell architecture was similar, except it could store 768 cache lines.
All 3 architectures do not use an LRU replacement policy, and have a cache line size of 32 bytes.
For shared memory, they found that all devices had 32 memory banks, with Fermi and Maxwell having a bank width of 4 bytes, and Kepler having a bank width of 8 bytes.
Interestingly, they also found that the efficiency of shared memory throughput was, 57.4\% on Fermi, 32.5\% on Kepler and 83.9\% on Maxwell.
Suggesting that a bank size of 4 bytes is optimal.
They also suggest that bank conflicts are the primary reason for poor memory latency when using shared memory on the GPU.

Virtual memory gives the illusion of a larger memory space than is physically available.
This design does not burden programmers with having to track physical memory.
GPUs now use multi-chip modules (MCM) to increase performance, however, this design can have implications on the virtual memory~\cite{Pratheek2023}.
Pratheek et al. explores the ideal design of a virtual memory system for a GPU that utilizes MCM.
They find that the partitioned nature of a MCM design leads to different design, and not one design works for all applications.
Mainly, the chief design consideration is to have a shared L2 TLB or a private L2 TLB.
Pratheek et al. proposed a MCM-aware virtual memory system which, "achieves a balance between using aggregate L2 TLB capacity, local L2 TLB accesses, and local page table entry accesses on a page walk.''~\cite{Pratheek2023}
This balance is achieved through intelligent scheduling of warps and data placement in MCM GPUs.

One of the problems with designing efficient address mapping schemes for GPUs is accounting for bit entropy~\cite{Liu2018}.
Liu et al.~\cite{Liu2018} found that the address bit entropy distribution did not follow a typical CPU bit entropy distribution.
Bit entropy is defined as high if the bit is likely to change and low if the bit is not likely to change.
Liu et al. notes that the entropy distribution is highly application dependent, this is in contrast to a CPU, in which typically the least significant bit is likely to have the highest entropy.
Additionally, these distributions tend to change while the application is running.
This problem is due to the GPUs highly parallelized architecture, in which thousands of threads might be accessing memory addresses concurrently.

The ideal memory addressing scheme would have row bits change the least, for row buffer locality, while the channel and bank bits should change the most, for uniform memory distribution~\cite{Liu2018}.
Liu et al. propsed the ``Page Address Entropy'' (PAE) mapping scheme, which fulfills these criteria, is easily translated into hardware with an array of XOR gates, and results in performance gains of around 1.25.

Due to the memory performance variability of applications on GPUs, it is often the case that algorithms need specific tuning to perform well on GPUs.
Liu et al.~\cite{Liu2020} shows this clearly with an example of how to increase performance of Non-deterministic Finite Automata (NFA). NFAs are naturally parallel, though they do not perform well on GPUs. Liu et al. found the main cause of this is excessive data movement and poor-compute utilization. This section will only talk about the data movement findings.

Liu et al. solved the data movement issue by privatizing reads using the GPUs on-chip memory.
They also made further optimizations that improve the NFA algorithm on the GPU by reducing redundant memory, reducing global memory accesses, and cleverly turning read operations into computations.
This example shows that, often algorithms that would theoretically perform well on the GPU (NFAs), need to be reworked in order to take full advantage of the GPUs highly parallel memory structure.

In summary, GPU memory systems share similarities with CPU memory systems, but also have stark differences.
Ideas like a hierarchy of memory are similar to a CPU, but the specific policies of the hierarchy of memory, such as a private or public L1 cache, are typically different than a CPU.
Much work has gone into finding the ideal memory system for GPUs, such as address mapping policies, write-through or write-back policies, coherent caches, and memory coalescing are all active areas of this research.
