\section{Architecture}
NVIDIA GPUs are designed to maximize parallel processing and consist of multiple SIMD pipelines called Shader Cores. The CUDA model utilizes GPUs as co-processors specialized in parallel computation, where threads grouped into CTAs (Cooperative Thread Arrays) collaborate by using shared memory. GPUs manage data through a hierarchical memory structure comprising L1 cache, shared memory, and global memory, with data transfer between Shader Cores and memory controllers managed by an interconnect network【\cite{Bakhoda2009】. The GPU scheduling mechanism supports various levels of parallelism, starting from the smallest unit, the warp (comprising 32 threads), progressing to CTAs (thread blocks), and finally, to CUDA streams at the highest level. Workloads are executed on each SM (Streaming Multiprocessor) and are queued to GPU resources for processing【\cite{Sanudo2020】.
GPU performance is often hindered by memory access bottlenecks, resource imbalance, and branch divergence. Inefficient handling of parallel memory requests within a warp leads to bottlenecks, while excessive CTA execution causes resource contention. Branch divergence occurs when threads within a warp execute different branches, reducing parallel processing efficiency. To address these issues, optimizing the scheduling algorithm can ensure a balanced workload distribution among SMs, and improving memory coalescing can enhance warp memory request efficiency. Additionally, introducing priority-based scheduling policies for real-time workloads can strengthen performance predictability【\cite{Bakhoda2009】【\cite{Sanudo2020】.
To mitigate branch divergence, the Thread Block Compaction (TBC) mechanism has been proposed. In the SIMT (Single-Instruction, Multiple-Thread) architecture of GPUs, multiple threads execute the same instruction in parallel. However, when branches occur, threads within a warp may follow different paths, leading to reduced efficiency. TBC leverages control-flow locality across an entire thread block to realign (compact) warps, grouping threads executing similar paths to enable parallel execution. This reduces idle resources and improves memory access efficiency. Experimental results show that TBC achieves an average speedup of 22% compared to the existing Dynamic Warp Formation (DWF) technique, demonstrating its effectiveness in alleviating bottlenecks【\cite{Fung2011HPC】. 
To further support parallel processing and performance optimization in NVIDIA GPUs, various tools and research have been proposed. One such tool is GASS, the first open-source LLVM-based compiler designed for NVIDIA GPUs. GASS transforms LLVM-IR into SASS (Specific Assembly) code and, unlike proprietary tools, provides detailed hardware modeling that includes GPU register sets, instruction sets, and latency-throughput characteristics【\cite{Yan2022】. This allows advanced optimizations such as instruction scheduling and if-conversion, enabling the maximization of parallel processing capabilities while considering the constraints of GPU hardware.
Another tool focuses on reverse-engineering NVIDIA GPUs' proprietary instruction set architecture (ISA) to analyze and decode their functionality. This framework supports various GPU architectures, including Fermi, Kepler, Maxwell, and Pascal【\cite{Hayes2019】. It generates assemblers that convert assembly code into binary instructions, enabling the study of GPU micro-architectural features such as memory management, control flow, and instruction scheduling. By bridging the gap between software and hardware, this tool facilitates performance optimization, security analysis, and deeper architectural understanding.
For addressing floating-point exceptions in GPU operations, the Xscope framework was developed. Floating-point exceptions (e.g., NaN, INF, underflow) are difficult to detect and predict in NVIDIA GPUs due to the absence of hardware flags for such issues【\cite{Laguna2022】. Xscope uses Bayesian optimization to explore input spaces for GPU functions, identifying inputs that cause exceptions in CUDA Math Library and high-performance computing (HPC) programs with a 72% success rate. This framework provides a crucial foundation for understanding and resolving floating-point issues in GPU architectures.
Lastly, significant advancements have been made in optimizing sparse neural networks on NVIDIA GPUs, particularly leveraging Tensor Cores. Sparse neural networks, which have many weights set to zero, often underperform on GPUs optimized for dense networks. To address this, the VectorSparse algorithm and hardware design enhance Tensor Core functionality to skip unnecessary zero computations and focus on essential operations【\cite{Zhu2019】. By extending Tensor Core instructions and optimizing data indexing and parallel processing, this approach achieves a 63% performance improvement for sparse networks and an additional 58% hardware-level efficiency boost compared to dense neural networks.
