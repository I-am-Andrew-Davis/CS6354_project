\section{Architecture}

NVIDIA GPUs are architected to maximize parallel processing, with the fundamental unit being the Shader Core. A Shader Core consists of multiple SIMD pipelines. The CUDA model utilizes the GPU as a co-processor specialized in parallel computation, where threads grouped into CTAs (Cooperative Thread Arrays) collaboratively perform tasks using shared memory.

GPUs manage data through a hierarchical memory structure composed of L1 cache, shared memory, and global memory. The fastest memory is the L1 cache connected to each Shader Core, which works alongside local and shared memory for rapid data processing. Shared memory within the Streaming Multiprocessor (SM) allows data exchange among CTAs within the SM, reducing data processing bottlenecks. Global memory, though slower, is accessible by all cores and serves as a large-capacity storage for handling extensive datasets. Data transfers occur via an interconnect network, ensuring efficient connectivity between Shader Cores and memory controllers ~\cite{Bakhoda2009}.

The GPU scheduling mechanism supports multiple levels of parallelism, starting with warps (groups of 32 threads) as the smallest unit, followed by CTAs (thread blocks) as the intermediate level, and CUDA streams at the top level. Workloads are executed within Streaming Multiprocessors (SMs), where tasks are queued and processed ~\cite{Sanudo2020}.

GPU performance can be hindered by memory access bottlenecks, resource imbalance, and branch divergence. Memory bottlenecks occur when parallel memory requests within a warp are inefficiently handled. Excessive CTA execution can lead to memory resource contention. Branch divergence arises when threads within a warp follow different branches, reducing parallel processing efficiency. To mitigate these issues, scheduling algorithms need to be optimized to evenly distribute tasks across SMs and to better coalesce memory requests.

The scheduling structure is divided into three main levels, each responsible for decisions at different granularities:
  1.Application Scheduler: At the top level, it manages task allocation across applications running on the GPU. Applications generate one or more channels and insert them into a runlist, where TDMA (Time Division Multiple Access) methods are used for scheduling based on priority and time slice length.
  2.Stream Scheduler: Within each application, it manages tasks submitted by the application across multiple CUDA streams. Tasks within a stream are executed in FIFO order, and CUDA allows the setting of stream priorities.
  3.Block Scheduler: Maps CUDA task blocks to individual Streaming Multiprocessors (SMs) to maximize parallelism. Work blocks are distributed primarily using a round-robin method. Within each SM, blocks are decomposed into warps, which are scheduled using the Loose Round Robin (LRR) approach. When a warp is stalled due to memory dependencies, another ready warp is scheduled. Key resources such as registers, shared memory, and warps are prioritized during block scheduling.
Parallel execution of tasks across streams can improve performance, but synchronization or resource contention complicates timing predictability ~\cite{Bakhoda2009}, ~\cite{Sanudo2020}.

To alleviate branch divergence, the Thread Block Compaction (TBC) mechanism has been proposed. TBC addresses divergence in the SIMT (Single-Instruction, Multiple-Thread) execution model by dynamically reorganizing threads within a block to group those following similar paths into new "compacted warps." These newly formed warps execute in parallel and restore the original static warp configuration at the next reconvergence point.

TBC replaces the traditional per-warp reconvergence stack with a block-wide reconvergence stack, enabling efficient synchronization and compaction by tracking control flow across all threads in a block. This mechanism minimizes idle threads, improves SIMD efficiency, and reduces memory divergence, thereby enhancing memory access efficiency. Additionally, TBC resolves scheduling inefficiencies and excessive memory conflicts encountered in prior methods like Dynamic Warp Formation (DWF). It eliminates complex hardware requirements and prevents execution bottlenecks such as starvation eddies.

Experiments show that TBC achieves a {22%} performance improvement over the per-warp stack mechanism (PDOM) and a {17\%} improvement over DWF, particularly excelling in applications with significant control flow divergence, such as graph traversal or molecular dynamics simulations. TBC is a robust and efficient solution that enhances GPU performance while minimizing hardware overhead ~\cite{Fung2011HPC}.

GASS, the pioneering open-source LLVM-based compiler, represents a significant milestone in GPU programming by enabling the conversion of LLVM-IR directly into SASS code for NVIDIA GPUs. This breakthrough addresses a longstanding limitation in the field, as previous tools like NVIDIA's proprietary ptxas were closed-source, limiting transparency and customizability. GASS offers compatibility across NVIDIA architectures, including Volta, Turing, and Ampere, creating an unprecedented platform for open experimentation and optimization. By modeling GPU hardware comprehensively, GASS facilitates fine-grained control over resource usage, supporting detailed representations of registers, vector predicates, and instruction sets. Its tailored instruction scheduling and if-conversion techniques further ensure efficient exploitation of computational parallelism and reduce data hazards by improving instruction latency and throughput ~\cite{Yan2022}.

The compiler's innovative contributions lie in its ability to directly address performance-critical aspects of GPU programming. For instance, if-conversion minimizes branch divergence by translating short branches into predicated instructions, boosting matrix multiplication throughput by up to {23\%}. Meanwhile, its advanced instruction scheduler optimizes compute-bound kernels, yielding performance gains of 5-{23\%} over existing solutions. These optimizations empower developers to explore new research directions, such as register allocation strategies, branch divergence analysis, and compile-time instrumentation. GASSâ€™s open-source nature democratizes GPU performance tuning and creates opportunities for deeper exploration of architectural nuances, making it an invaluable tool for both academic and industrial use in high-performance computing and machine learning applications ~\cite{Yan2022}.

Hayes et al. outlined a framework for decoding NVIDIA GPU's closed ISA (Instruction Set Architecture), designed to support various architectures such as Fermi, Kepler, Maxwell, and Pascal. The framework enables bidirectional conversion between binary instructions and assembly code, systematically decoding the encoding structure of GPU instructions. This capability allows researchers to analyze microarchitectural features like instruction scheduling, memory management, and control flow in detail. Using automated ISA analysis and bit-flipping techniques, it detects instruction variations and generates assemblers compatible with newer GPU generations. This approach overcomes the limitations of traditional ISA analysis, providing broad GPU architecture compatibility ~\cite{Hayes2019}.

The framework bridges the gap between GPU software and hardware, playing a vital role in performance optimization, security analysis, and architectural research. Decoded ISAs facilitate tasks like performance tuning, binary modification, and instruction scheduling optimization, including converting local memory instructions to shared memory instructions or reconfiguring code blocks. This helps overcome research constraints imposed by NVIDIA GPU's closed architecture, supporting next-generation GPU hardware and software development.

Laguna et al. proposed Xscope, a novel framework for identifying floating-point exceptions caused by input values in NVIDIA GPUs. While GPUs excel at parallel computation, tools to detect or predict exceptions in floating-point operations (e.g., NaN, INF, underflow) are limited. NVIDIA GPUs lack hardware flags for exception detection. Xscope uses Bayesian Optimization to explore input value spaces of GPU functions, achieving a {72\%} success rate in identifying exception-causing inputs for CUDA Math Library and HPC programs. This provides insights into floating-point issues in GPU architectures and offers solutions ~\cite{Laguna2022}.

Zhu et al. (2019) proposed a solution to improve the computational efficiency of neural networks by enabling faster execution of sparse neural networks on GPUs. Sparse neural networks reduce the computational workload by eliminating unnecessary values (zeros). However, GPUs are primarily optimized for dense neural networks, which consist mostly of non-zero values, leading to suboptimal performance for sparse networks. To address this, the researchers developed a novel algorithm called VectorSparse. This algorithm overcomes the limitation of dense networks, which lack removable values, and improves the computational efficiency of sparse networks. The method involves dividing the data into smaller vectors and removing values at the same ratio within each vector. Instead of simply discarding small numbers, the algorithm retains the most important values in each vector while setting the rest to zero. This approach maintains balanced sparsity, preventing workload imbalances that typically arise in GPUs when processing sparse networks in parallel ~\cite{Zhu2019}.

In addition, the researchers enhanced Tensor Core, a specialized hardware component in modern GPUs, to maximize the efficiency of sparse neural networks. Tensor Core is primarily optimized for dense data, but the team modified its instruction set and architecture to skip zeros and compute only the necessary values for sparse data. As a result, the algorithm alone achieved a {63\%} performance improvement over dense neural networks, and further hardware optimizations provided an additional {58\%} speedup. This study demonstrates a promising approach to processing complex neural networks more efficiently, highlighting significant potential for applications in machine learning and related fields~\cite{Zhu2019}.

