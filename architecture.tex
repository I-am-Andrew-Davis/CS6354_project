\section{Architecture}

NVIDIA GPUs are architected to maximize parallel processing, with the fundamental unit being the Shader Core. A Shader Core consists of multiple SIMD pipelines. The CUDA model utilizes the GPU as a co-processor specialized in parallel computation, where threads grouped into CTAs (Cooperative Thread Arrays) collaboratively perform tasks using shared memory.

GPUs manage data through a hierarchical memory structure composed of L1 cache, shared memory, and global memory. The fastest memory is the L1 cache connected to each Shader Core, which works alongside local and shared memory for rapid data processing. Shared memory within the Streaming Multiprocessor (SM) allows data exchange among CTAs within the SM, reducing data processing bottlenecks. Global memory, though slower, is accessible by all cores and serves as a large-capacity storage for handling extensive datasets. Data transfers occur via an interconnect network, ensuring efficient connectivity between Shader Cores and memory controllers ~\cite{Bakhoda2009}.

The GPU scheduling mechanism supports multiple levels of parallelism, starting with warps (groups of 32 threads) as the smallest unit, followed by CTAs (thread blocks) as the intermediate level, and CUDA streams at the top level. Workloads are executed within Streaming Multiprocessors (SMs), where tasks are queued and processed ~\cite{Sanudo2020}.

GPU performance can be hindered by memory access bottlenecks, resource imbalance, and branch divergence. Memory bottlenecks occur when parallel memory requests within a warp are inefficiently handled. Excessive CTA execution can lead to memory resource contention. Branch divergence arises when threads within a warp follow different branches, reducing parallel processing efficiency. To mitigate these issues, scheduling algorithms need to be optimized to evenly distribute tasks across SMs and to better coalesce memory requests.

The scheduling structure is divided into three main levels, each responsible for decisions at different granularities:

Application Scheduler: At the top level, it manages task allocation across applications running on the GPU. Applications generate one or more channels and insert them into a runlist, where TDMA (Time Division Multiple Access) methods are used for scheduling based on priority and time slice length.

Stream Scheduler: Within each application, it manages tasks submitted by the application across multiple CUDA streams. Tasks within a stream are executed in FIFO order, and CUDA allows the setting of stream priorities.

Block Scheduler: Maps CUDA task blocks to individual Streaming Multiprocessors (SMs) to maximize parallelism. Work blocks are distributed primarily using a round-robin method. Within each SM, blocks are decomposed into warps, which are scheduled using the Loose Round Robin (LRR) approach. When a warp is stalled due to memory dependencies, another ready warp is scheduled. Key resources such as registers, shared memory, and warps are prioritized during block scheduling.

Parallel execution of tasks across streams can improve performance, but synchronization or resource contention complicates timing predictability ~\cite{Bakhoda2009}, ~\cite{Sanudo2020}.

To alleviate branch divergence, the Thread Block Compaction (TBC) mechanism has been proposed. TBC addresses divergence in the SIMT (Single-Instruction, Multiple-Thread) execution model by dynamically reorganizing threads within a block to group those following similar paths into new "compacted warps." These newly formed warps execute in parallel and restore the original static warp configuration at the next reconvergence point.

TBC replaces the traditional per-warp reconvergence stack with a block-wide reconvergence stack, enabling efficient synchronization and compaction by tracking control flow across all threads in a block. This mechanism minimizes idle threads, improves SIMD efficiency, and reduces memory divergence, thereby enhancing memory access efficiency. Additionally, TBC resolves scheduling inefficiencies and excessive memory conflicts encountered in prior methods like Dynamic Warp Formation (DWF). It eliminates complex hardware requirements and prevents execution bottlenecks such as starvation eddies.

Experiments show that TBC achieves a 22% performance improvement over the per-warp stack mechanism (PDOM) and a 17% improvement over DWF, particularly excelling in applications with significant control flow divergence, such as graph traversal or molecular dynamics simulations. TBC is a robust and efficient solution that enhances GPU performance while minimizing hardware overhead ~\cite{Fung2011HPC}.

Yan et al. introduced GASS, the first open-source compiler to convert LLVM-IR to SASS (Specific Assembly) code, based on NVIDIA GPU hardware modeling. Previous tools were proprietary, making it difficult for users to fine-tune hardware or optimize performance. GASS models GPU hardware in detail, including register sets (255 32-bit registers and vector condition registers), instruction sets, and scheduling models that reflect instruction latency and throughput. This modeling facilitates optimization of parallel processing structures and resource usage patterns, especially through instruction scheduling and if-conversion to prevent data hazards and improve performance. GASS maximizes computational kernel parallelism while generating efficient code within GPU hardware constraints ~\cite{Yan2022}.

Hayes et al. outlined a framework for decoding NVIDIA GPU's closed ISA (Instruction Set Architecture), designed to support various architectures such as Fermi, Kepler, Maxwell, and Pascal. The framework enables bidirectional conversion between binary instructions and assembly code, systematically decoding the encoding structure of GPU instructions. This capability allows researchers to analyze microarchitectural features like instruction scheduling, memory management, and control flow in detail. Using automated ISA analysis and bit-flipping techniques, it detects instruction variations and generates assemblers compatible with newer GPU generations. This approach overcomes the limitations of traditional ISA analysis, providing broad GPU architecture compatibility ~\cite{Hayes2019}.

The framework bridges the gap between GPU software and hardware, playing a vital role in performance optimization, security analysis, and architectural research. Decoded ISAs facilitate tasks like performance tuning, binary modification, and instruction scheduling optimization, including converting local memory instructions to shared memory instructions or reconfiguring code blocks. This helps overcome research constraints imposed by NVIDIA GPU's closed architecture, supporting next-generation GPU hardware and software development.

Laguna et al. proposed Xscope, a novel framework for identifying floating-point exceptions caused by input values in NVIDIA GPUs. While GPUs excel at parallel computation, tools to detect or predict exceptions in floating-point operations (e.g., NaN, INF, underflow) are limited. NVIDIA GPUs lack hardware flags for exception detection. Xscope uses Bayesian Optimization to explore input value spaces of GPU functions, achieving a 72% success rate in identifying exception-causing inputs for CUDA Math Library and HPC programs. This provides insights into floating-point issues in GPU architectures and offers solutions ~\cite{Laguna2022}.

Zhu et al. highlighted Tensor Core's role in enhancing the performance of sparse neural networks on NVIDIA GPUs. Sparse neural networks, characterized by weights pruned to zero, face performance challenges on GPUs optimized for dense neural networks. To address this, the VectorSparse algorithm and hardware design balance the workload of sparse neural networks, leveraging Tensor Core to skip zeros and rapidly compute essential values. This approach extends the Tensor Core instruction set and optimizes data indexing and parallel processing, achieving a 63% performance improvement over dense neural networks and an additional 58% hardware-based performance boost ~\cite{Zhu2019}.
