%Reference 1
@INPROCEEDINGS{Bakhoda2009,
  author={Bakhoda, Ali and Yuan, George L. and Fung, Wilson W. L. and Wong, Henry and Aamodt, Tor M.},
  booktitle={2009 IEEE International Symposium on Performance Analysis of Systems and Software},
  title={Analyzing CUDA workloads using a detailed GPU simulator},
  year={2009},
  volume={},
  number={},
  pages={163-174},
  keywords={Analytical models;Yarn;Graphics;Parallel processing;Microarchitecture;Hardware;Process design;Concurrent computing;Parallel programming;Computational modeling},
  doi={10.1109/ISPASS.2009.4919648}}
%Reference 2
@INPROCEEDINGS{Fung2011HPC,
  author={Fung, Wilson W. L. and Aamodt, Tor M.},
  booktitle={2011 IEEE 17th International Symposium on High Performance Computer Architecture},
  title={Thread block compaction for efficient SIMT control flow},
  year={2011},
  volume={},
  number={},
  pages={25-36},
  keywords={Instruction sets;Graphics processing unit;Compaction;Hardware;Kernel;Pipelines;Random access memory},
  doi={10.1109/HPCA.2011.5749714}}
%Reference 3
@INPROCEEDINGS{Singh2013,
  author={Singh, Inderpreet and Shriraman, Arrvindh and Fung, Wilson W. L. and O'Connor, Mike and Aamodt, Tor M.},
  booktitle={2013 IEEE 19th International Symposium on High Performance Computer Architecture (HPCA)},
  title={Cache coherence for GPU architectures},
  year={2013},
  volume={},
  number={},
  pages={578-590},
  keywords={Coherence;Graphics processing units;Protocols;Instruction sets;Complexity theory;Transient analysis;Synchronization},
  doi={10.1109/HPCA.2013.6522351}}
%Reference 4
@INPROCEEDINGS{Khairy2020,
  author={Khairy, Mahmoud and Shen, Zhesheng and Aamodt, Tor M. and Rogers, Timothy G.},
  booktitle={2020 ACM/IEEE 47th Annual International Symposium on Computer Architecture (ISCA)},
  title={Accel-Sim: An Extensible Simulation Framework for Validated GPU Modeling},
  year={2020},
  volume={},
  number={},
  pages={473-486},
  keywords={GPGPU;Modeling and Simulation},
  doi={10.1109/ISCA45697.2020.00047}}
%Reference 5
@INPROCEEDINGS{Kandiah2021,
author = {Kandiah, Vijay and Peverelle, Scott and Khairy, Mahmoud and Pan, Junrui and Manjunath, Amogh and Rogers, Timothy G. and Aamodt, Tor M. and Hardavellas, Nikos},
title = {AccelWattch: A Power Modeling Framework for Modern GPUs},
year = {2021},
isbn = {9781450385572},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3466752.3480063},
doi = {10.1145/3466752.3480063},
abstract = {Graphics Processing Units (GPUs) are rapidly dominating the accelerator space, as illustrated by their wide-spread adoption in the data analytics and machine learning markets. At the same time, performance per watt has emerged as a crucial evaluation metric together with peak performance. As such, GPU architects require robust tools that will enable them to model both the performance and the power consumption of modern GPUs. However, while GPU performance modeling has progressed in great strides, power modeling has lagged behind. To mitigate this problem we propose AccelWattch, a configurable GPU power model that resolves two long-standing needs: the lack of a detailed and accurate cycle-level power model for modern GPU architectures, and the inability to capture their constant and static power with existing tools. AccelWattch can be driven by emulation and trace-driven environments, hardware counters, or a mix of the two, models both PTX and SASS ISAs, accounts for power gating and control-flow divergence, and supports DVFS. We integrate AccelWattch with GPGPU-Sim and Accel-Sim to facilitate its widespread use. We validate AccelWattch on a NVIDIA Volta GPU, and show that it achieves strong correlation against hardware power measurements. Finally, we demonstrate that AccelWattch can enable reliable design space exploration: by directly applying AccelWattch tuned for Volta on GPU configurations resembling NVIDIA Pascal and Turing GPUs, we obtain accurate power models for these architectures.},
booktitle = {MICRO-54: 54th Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {738–753},
numpages = {16},
keywords = {Power Modeling and Simulation, GPGPU/GPU Computing},
location = {Virtual Event, Greece},
series = {MICRO '21}}
%Reference 6
@inproceedings{Fung2011ISM,
author = {Fung, Wilson W. L. and Singh, Inderpreet and Brownsword, Andrew and Aamodt, Tor M.},
title = {Hardware transactional memory for GPU architectures},
year = {2011},
isbn = {9781450310536},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2155620.2155655},
doi = {10.1145/2155620.2155655},
abstract = {Graphics processor units (GPUs) are designed to efficiently exploit thread level parallelism (TLP), multiplexing execution of 1000s of concurrent threads on a relatively smaller set of single-instruction, multiple-thread (SIMT) cores to hide various long latency operations. While threads within a CUDA block/OpenCL workgroup can communicate efficiently through an intra-core scratchpad memory, threads in different blocks can only communicate via global memory accesses. Programmers wishing to exploit such communication have to consider data-races that may occur when multiple threads modify the same memory location. Recent GPUs provide a form of inter-block communication through atomic operations for single 32-bit/64-bit words. Although fine-grained locks can be constructed from these atomic operations, synchronization using locks is prone to deadlock. In this paper, we propose to solve these problems by extending GPUs to support transactional memory (TM). Major challenges include supporting 1000s of concurrent transactions and committing non-conflicting transactions in parallel. We propose KILO TM, a novel hardware TM design for GPUs that scales to 1000s of concurrent transactions. Without cache coherency hardware to depend on, it uses word-level, value-based conflict detection to avoid broadcast communication and reduce on-chip storage overhead. It employs speculative validation using a novel bloom filter organization to increase transaction commit parallelism. For a set of TM-enhanced GPU applications, KILO TM captures 59\% of the performance of fine-grained locking, and is on average 128x faster than executing all transactions serially, for an estimated hardware area overhead of 0.5\% of a commercial GPU.},
booktitle = {Proceedings of the 44th Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {296–307},
numpages = {12},
location = {Porto Alegre, Brazil},
series = {MICRO-44}
}
%Reference 7
@article{Raihan2018,
  author       = {Md Aamir Raihan and
                  Negar Goli and
                  Tor M. Aamodt},
  title        = {Modeling Deep Learning Accelerator Enabled GPUs},
  journal      = {CoRR},
  volume       = {abs/1811.08309},
  year         = {2018},
  url          = {http://arxiv.org/abs/1811.08309},
  eprinttype    = {arXiv},
  eprint       = {1811.08309},
  timestamp    = {Mon, 26 Nov 2018 12:52:45 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1811-08309.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}}
%Reference 8
@inproceedings{Zhu2019,
author = {Zhu, Maohua and Zhang, Tao and Gu, Zhenyu and Xie, Yuan},
title = {Sparse Tensor Core: Algorithm and Hardware Co-Design for Vector-wise Sparse Neural Networks on Modern GPUs},
year = {2019},
isbn = {9781450369381},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3352460.3358269},
doi = {10.1145/3352460.3358269},
abstract = {Deep neural networks have become the compelling solution for the applications such as image classification, object detection, speech recognition, and machine translation. However, the great success comes at the cost of excessive computation due to the over-provisioned parameter space. To improve the computation efficiency of neural networks, many pruning techniques have been proposed to reduce the amount of multiply-accumulate (MAC) operations, which results in high sparsity in the networks.Unfortunately, the sparse neural networks often run slower than their dense counterparts on modern GPUs due to their poor device utilization rate. In particular, as the sophisticated hardware primitives (e.g., Tensor Core) have been deployed to boost the performance of dense matrix multiplication by an order of magnitude, the performance of sparse neural networks lags behind significantly.In this work, we propose an algorithm and hardware co-design methodology to accelerate the sparse neural networks. A novel pruning algorithm is devised to improve the workload balance and reduce the decoding overhead of the sparse neural networks. Meanwhile, new instructions and micro-architecture optimization are proposed in Tensor Core to adapt to the structurally sparse neural networks. Our experimental results show that the pruning algorithm can achieve 63\% performance gain with model accuracy sustained. Furthermore, the hardware optimization gives an additional 58\% performance gain with negligible area overhead.},
booktitle = {Proceedings of the 52nd Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {359–371},
numpages = {13},
keywords = {graphics processing units, neural networks, pruning},
location = {Columbus, OH, USA},
series = {MICRO '52}}
%Reference 9
@inproceedings{Yan2022,
author = {Yan, Da and Wang, Wei and Chu, Xiaowen},
title = {An LLVM-based open-source compiler for NVIDIA GPUs},
year = {2022},
isbn = {9781450392044},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3503221.3508428},
doi = {10.1145/3503221.3508428},
abstract = {We present GASS, an LLVM-based open-source compiler for NVIDIA GPU's SASS machine assembly. GASS is the first open-source compiler targeting SASS, and it provides a unified toolchain for currently fragmented low-level performance research on NVIDIA GPUs. GASS supports all recent architectures, including Volta, Turing, and Ampere.Our evaluation shows that our specialized optimizations deliver significant speedup over LLVM's algorithms.},
booktitle = {Proceedings of the 27th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
pages = {448–449},
numpages = {2},
keywords = {GPU, LLVM, SASS, compiler},
location = {Seoul, Republic of Korea},
series = {PPoPP '22}}
%Reference 10
@inproceedings{Hayes2019,
author = {Hayes, Ari B. and Hua, Fei and Huang, Jin and Chen, Yanhao and Zhang, Eddy Z.},
title = {Decoding CUDA binary},
year = {2019},
isbn = {9781728114361},
publisher = {IEEE Press},
abstract = {NVIDIA's software does not offer translation of assembly code to binary for their GPUs, since the specifications are closed-source. This work fills that gap. We develop a systematic method of decoding the Instruction Set Architectures (ISAs) of NVIDIA's GPUs, and generating assemblers for different generations of GPUs. Our framework enables cross-architecture binary analysis and transformation. Making the ISA accessible in this manner opens up a world of opportunities for developers and researchers, enabling numerous optimizations and explorations that are unachievable at the source-code level. Our infrastructure has already benefited and been adopted in important applications including performance tuning, binary instrumentation, resource allocation, and memory protection.},
booktitle = {Proceedings of the 2019 IEEE/ACM International Symposium on Code Generation and Optimization},
pages = {229–241},
numpages = {13},
keywords = {CUDA, Code Generation, Code Translation and Transformation, GPU, Instruction Set Architecture (ISA)},
location = {Washington, DC, USA},
series = {CGO 2019}}
%Reference 11
@inproceedings{Pratheek2023,
author = {B, Pratheek and Jawalkar, Neha and Basu, Arkaprava},
title = {Designing Virtual Memory System of MCM GPUs},
year = {2023},
isbn = {9781665462723},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO56248.2022.00036},
doi = {10.1109/MICRO56248.2022.00036},
abstract = {Multi-Chip Module (MCM) designs have emerged as a key technique to scale up a GPU's compute capabilities in the face of slowing transistor technology. However, the disaggregated nature of MCM GPUs with many chiplets connected via in-package interconnects leads to non-uniformity.We explore the implications of MCM's non-uniformity on the GPU's virtual memory. We quantitatively demonstrate that an MCM-aware virtual memory system should aim to 1 leverage aggregate TLB capacity across chiplets while limiting accesses to L2 TLB on remote chiplets, 2 reduce accesses to page table entries resident on a remote chiplet's memory during page walks. We propose MCM-aware GPU virtual memory (MGvm) that leverages static analysis techniques, previously used for thread and data placement, to map virtual addresses to chiplets and to place the page tables. At runtime, MGvm balances its objective of limiting the number of remote L2 TLB lookups with that of reducing the number of remote page table accesses to achieve good speedups (52\%, on average) across diverse application behaviors.},
booktitle = {Proceedings of the 55th Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {404–422},
numpages = {19},
keywords = {graphics processing units, multi-chip module, chiplet, virtual memory, address translation, page table walkers, translation look-aside buffers},
location = {Chicago, Illinois, USA},
series = {MICRO '22}}
%Reference 12
@article{Mei2015,
  author       = {Xinxin Mei and
                  Xiaowen Chu},
  title        = {Dissecting {GPU} Memory Hierarchy through Microbenchmarking},
  journal      = {CoRR},
  volume       = {abs/1509.02308},
  year         = {2015},
  url          = {http://arxiv.org/abs/1509.02308},
  eprinttype    = {arXiv},
  eprint       = {1509.02308},
  timestamp    = {Wed, 13 Jul 2022 14:54:31 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/MeiC15.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}}
%Reference 13
@inproceedings{Sanudo2020,
author = {Sañudo, Ignacio and Capodieci, Nicola and Martinez, Jorge and Marongiu, Andrea and Bertogna, Marko},
year = {2020},
month = {04},
pages = {},
title = {Dissecting the CUDA scheduling hierarchy: a Performance and Predictability Perspective},
doi = {10.1109/RTAS48715.2020.000-5}}
%Reference 14
@inproceedings{Laguna2022,
author = {Laguna, Ignacio and Gopalakrishnan, Ganesh},
title = {Finding inputs that trigger floating-point exceptions in GPUs via bayesian optimization},
year = {2022},
isbn = {9784665454445},
publisher = {IEEE Press},
abstract = {Testing code for floating-point exceptions is crucial as exceptions can quickly propagate and produce unreliable numerical answers. The state-of-the-art to test for floating-point exceptions in GPUs is quite limited and solutions require the application's source code, which precludes their use in accelerated libraries where the source is not publicly available. We present an approach to find inputs that trigger floating-point exceptions in black-box GPU functions, i.e., functions where the source code and information about input bounds are unavailable. Our approach is the first to use Bayesian optimization (BO) to identify such inputs and uses novel strategies to overcome the challenges that arise in applying BO to this problem. We implement our approach in the Xscope framework and demonstrate it on 58 functions from the CUDA Math Library and functions from ten HPC programs. Xscope is able to identify inputs that trigger exceptions in about 72\% of the tested functions.},
booktitle = {Proceedings of the International Conference on High Performance Computing, Networking, Storage and Analysis},
articleno = {33},
numpages = {14},
keywords = {floating-point exceptions, bayesian optimization, GPU computing},
location = {Dallas, Texas},
series = {SC '22}}
%Reference 15
@INPROCEEDINGS{Liu2018,
  author={Liu, Yuxi and Zhao, Xia and Jahre, Magnus and Wang, Zhenlin and Wang, Xiaolin and Luo, Yingwei and Eeckhout, Lieven},
  booktitle={2018 ACM/IEEE 45th Annual International Symposium on Computer Architecture (ISCA)},
  title={Get Out of the Valley: Power-Efficient Address Mapping for GPUs},
  year={2018},
  volume={},
  number={},
  pages={166-179},
  keywords={Entropy;Instruction sets;Graphics processing units;Random access memory;Hardware;Measurement;Organizations;GPU;Address Mapping;Power Efficiency;Memory Systems},
  doi={10.1109/ISCA.2018.00024}}
%Reference 16
@inproceedings{Jog2013,
author = {Jog, Adwait and Kayiran, Onur and Mishra, Asit K. and Kandemir, Mahmut T. and Mutlu, Onur and Iyer, Ravishankar and Das, Chita R.},
title = {Orchestrated scheduling and prefetching for GPGPUs},
year = {2013},
isbn = {9781450320795},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2485922.2485951},
doi = {10.1145/2485922.2485951},
abstract = {In this paper, we present techniques that coordinate the thread scheduling and prefetching decisions in a General Purpose Graphics Processing Unit (GPGPU) architecture to better tolerate long memory latencies. We demonstrate that existing warp scheduling policies in GPGPU architectures are unable to effectively incorporate data prefetching. The main reason is that they schedule consecutive warps, which are likely to access nearby cache blocks and thus prefetch accurately for one another, back-to-back in consecutive cycles. This either 1) causes prefetches to be generated by a warp too close to the time their corresponding addresses are actually demanded by another warp, or 2) requires sophisticated prefetcher designs to correctly predict the addresses required by a future "far-ahead" warp while executing the current warp.We propose a new prefetch-aware warp scheduling policy that overcomes these problems. The key idea is to separate in time the scheduling of consecutive warps such that they are not executed back-to-back. We show that this policy not only enables a simple prefetcher to be effective in tolerating memory latencies but also improves memory bank parallelism, even when prefetching is not employed. Experimental evaluations across a diverse set of applications on a 30-core simulated GPGPU platform demonstrate that the prefetch-aware warp scheduler provides 25\% and 7\% average performance improvement over baselines that employ prefetching in conjunction with, respectively, the commonly-employed round-robin scheduler or the recently-proposed two-level warp scheduler. Moreover, when prefetching is not employed, the prefetch-aware warp scheduler provides higher performance than both of these baseline schedulers as it better exploits memory bank parallelism.},
booktitle = {Proceedings of the 40th Annual International Symposium on Computer Architecture},
pages = {332–343},
numpages = {12},
keywords = {warp scheduling, prefetching, latency tolerance, GPGPUs},
location = {Tel-Aviv, Israel},
series = {ISCA '13}
}
%Reference 17
@inproceedings{10.1145/2451116.2451158,
author = {Jog, Adwait and Kayiran, Onur and Chidambaram Nachiappan, Nachiappan and Mishra, Asit K. and Kandemir, Mahmut T. and Mutlu, Onur and Iyer, Ravishankar and Das, Chita R.},
title = {OWL: cooperative thread array aware scheduling techniques for improving GPGPU performance},
year = {2013},
isbn = {9781450318709},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2451116.2451158},
doi = {10.1145/2451116.2451158},
abstract = {Emerging GPGPU architectures, along with programming models like CUDA and OpenCL, offer a cost-effective platform for many applications by providing high thread level parallelism at lower energy budgets. Unfortunately, for many general-purpose applications, available hardware resources of a GPGPU are not efficiently utilized, leading to lost opportunity in improving performance. A major cause of this is the inefficiency of current warp scheduling policies in tolerating long memory latencies.In this paper, we identify that the scheduling decisions made by such policies are agnostic to thread-block, or cooperative thread array (CTA), behavior, and as a result inefficient. We present a coordinated CTA-aware scheduling policy that utilizes four schemes to minimize the impact of long memory latencies. The first two schemes, CTA-aware two-level warp scheduling and locality aware warp scheduling, enhance per-core performance by effectively reducing cache contention and improving latency hiding capability. The third scheme, bank-level parallelism aware warp scheduling, improves overall GPGPU performance by enhancing DRAM bank-level parallelism. The fourth scheme employs opportunistic memory-side prefetching to further enhance performance by taking advantage of open DRAM rows. Evaluations on a 28-core GPGPU platform with highly memory-intensive applications indicate that our proposed mechanism can provide 33\% average performance improvement compared to the commonly-employed round-robin warp scheduling policy.},
booktitle = {Proceedings of the Eighteenth International Conference on Architectural Support for Programming Languages and Operating Systems},
pages = {395–406},
numpages = {12},
keywords = {GPGPUs, latency tolerance, prefetching, scheduling},
location = {Houston, Texas, USA},
series = {ASPLOS '13}
}
%Reference 18
@article{Kadam2018,
  title={RCoal: Mitigating GPU Timing Attack via Subwarp-Based Randomized Coalescing Techniques},
  author={Gurunath Kadam and Danfeng Zhang and Adwait Jog},
  journal={2018 IEEE International Symposium on High Performance Computer Architecture (HPCA)},
  year={2018},
  pages={156-167},
  url={https://api.semanticscholar.org/CorpusID:4565978}}
%Reference 19
@inproceedings{Liu2020,
author = {Liu, Hongyuan and Pai, Sreepathi and Jog, Adwait},
title = {Why GPUs are Slow at Executing NFAs and How to Make them Faster},
year = {2020},
isbn = {9781450371025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3373376.3378471},
doi = {10.1145/3373376.3378471},
abstract = {Non-deterministic Finite Automata (NFA) are space-efficient finite state machines that have significant applications in domains such as pattern matching and data analytics. In this paper, we investigate why the Graphics Processing Unit (GPU)---a massively parallel computational device with the highest memory bandwidth available on general-purpose processors---cannot efficiently execute NFAs. First, we identify excessive data movement in the GPU memory hierarchy and describe how to privatize reads effectively using GPU's on-chip memory hierarchy to reduce this excessive data movement. We also show that in several cases, indirect table lookups in NFAs can be eliminated by converting memory reads into computation, to further reduce the number of memory reads. Although our optimization techniques significantly alleviate these memory-related bottlenecks, a side effect of these techniques is the static assignment of work to cores. This leads to poor compute utilization, where GPU cores are wasted on idle NFA states. Therefore, we propose a new dynamic scheme that effectively balances compute utilization with reduced memory usage. Our combined optimizations provide a significant improvement over the previous state-of-the-art GPU implementations of NFAs. Moreover, they enable current GPUs to outperform the domain-specific accelerator for NFAs (i.e., Automata Processor) across several applications while performing within an order of magnitude for the rest of the applications.},
booktitle = {Proceedings of the Twenty-Fifth International Conference on Architectural Support for Programming Languages and Operating Systems},
pages = {251–265},
numpages = {15},
keywords = {finite state machine, gpu, parallel computing},
location = {Lausanne, Switzerland},
series = {ASPLOS '20}}
%Reference 20
@inproceedings{Ibrahim2020,
author = {Ibrahim, Mohamed Assem and Kayiran, Onur and Eckert, Yasuko and Loh, Gabriel H. and Jog, Adwait},
title = {Analyzing and Leveraging Shared L1 Caches in GPUs},
year = {2020},
isbn = {9781450380751},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3410463.3414623},
doi = {10.1145/3410463.3414623},
abstract = {Graphics Processing Units (GPUs) concurrently execute thousands of threads, which makes them effective for achieving high throughput for a wide range of applications. However, the memory wall often limits peak throughput. GPUs use caches to address this limitation, and hence several prior works have focused on improving cache hit rates, which in turn can improve throughput for memory-intensive applications. However, almost all of the prior works assume a conventional cache hierarchy where each GPU core has a private local L1 cache and all cores share the L2 cache. Our analysis shows that this canonical organization does not allow optimal utilization of caches because the private nature of L1 caches allows multiple copies of the same cache line to get replicated across cores.We introduce a new shared L1 cache organization, where all cores collectively cache a single copy of the data at only one location (core), leading to zero data replication. We achieve this by allowing each core to cache only a non-overlapping slice of the entire address range. Such a design is useful for significantly improving the collective L1 hit rates but incurs latency overheads from additional communications when a core requests data not allowed to be present in its own cache. While many workloads can tolerate this additional latency, several workloads show performance sensitivities. Therefore, we develop lightweight communication optimization techniques and a run-time mechanism that considers the latency-tolerance characteristics of applications to decide which applications should execute in private versus shared L1 cache organization and reconfigures the caches accordingly. In effect, we achieve significant performance and energy efficiency improvements, at a modest hardware cost, for applications that prefer the shared organization, with little to no impact on other applications.},
booktitle = {Proceedings of the ACM International Conference on Parallel Architectures and Compilation Techniques},
pages = {161–173},
numpages = {13},
keywords = {locality, gpus, bandwidth},
location = {Virtual Event, GA, USA},
series = {PACT '20}}
